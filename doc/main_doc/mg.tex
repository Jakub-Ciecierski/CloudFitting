\documentclass{book}

% PACKAGES
%--------------------------------------------------------------------------%
\usepackage[utf8]{inputenc}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{wrapfig}

\usepackage{csquotes}
\usepackage{amsmath}
\usepackage{epigraph}
\usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage{adjustbox}

\DeclareMathOperator*{\argmin}{arg\,min} %argmin
\DeclareMathOperator*{\argmax}{arg\,max} %argmax

\usepackage{floatrow}
% Table float box with bottom caption, box width adjusted to content
\newfloatcommand{capbtabbox}{table}[][\FBwidth]
\usepackage{caption}
\usepackage{subcaption}
\captionsetup{compatibility=false}

%%%

\usepackage{array}
% For tabulars
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}


\usepackage{pdfpages}
\usepackage{graphicx}
\usepackage{longtable}
%\usepackage[export]{adjustbox}
%\usepackage{tabu}
\usepackage{xcolor,colortbl}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{amsfonts}

% LOAD CUSTOM COMMANDS FROM A FILE
%--------------------------------------------------------------------------%
%\usepackage{result_figures}


% TITLE
%--------------------------------------------------------------------------%


\author{Jakub Ciecierski}
\date{\today}



% DOCUMENT
%--------------------------------------------------------------------------%

\begin{document}


\begin{titlepage}
    \begin{center}

            \textbf{\Huge Projektowanie Systemów CAD/CAM} \\ {\huge Cloud Fitting.} \\ [0.5cm]

            \vspace*{\fill}

            \textbf{\large Jakub Ciecierski \\ Anna Witwicka \\ Lukasz Wójcik}

            \vspace*{\fill}
            
            \textnormal{\large Warsaw University of Technology \\ Factulty of Mathematics and Information Science. \\ \today}

    \end{center}
\end{titlepage}

\tableofcontents


\chapter*{Introduction}

\chapter{Algorithms}

The algorithm is supposed to fit a cloud of points to an object of interest. We are interested in 3 dimensional space.
The figure~\ref{fig:cloud_fitting} shows example of a sphere mesh (green) and cloud of points extracted from its net (red). The cloud was generated by randomly selecting points from the sphere net and distorting them with normal distribution (TODO).

Once, we have a cloud and an object, we can input the pair to fitting algorithm.


\begin{figure}[H]
    \includegraphics[width=1\textwidth]{./graphics/cloud_fitting_init.png}
    \includegraphics[width=1\textwidth]{./graphics/cloud_fitting_after.png}
    \caption{Example of Cloud Fitting. Top picture presents the initial state, the lower picture shows fitted cloud.}
    \label{fig:cloud_fitting}
\end{figure}

%---------------------------------------------------------------
\chapter{Particle Swarm Optimization}\label{chap:pso}

The Particle Swarm Optimization (PSO) is population based stochastic optimization algorithm, originated by Eberhart and Kennedy~\cite{pso_origin}. Swarm of particles fly through the search space in order to find the most optimal solution of a given problem. The particles remember the best found solution and the further search is affected by it.

Each particle consists of the several fields. The position $X_p$ and velocity vector $V_p$ lets the particle travel through the search space. $Fitness$ value which is evaluated by the fitness function, represents the quality of the solution. The particle also remembers the best fitness value computed so far. Finally, two additional positions called $pbest_p$ and $lbest_p$ are stored. The position in which the particle $p$ reached its best fitness value so far is called $pbest_p$. However, the $lbest_p$ is the position of the best fitness value obtained so far by any other particle in the neighbourhood of particle $p$. The concept of neighbourhood is defined later in this chapter.

PSO is initialized with random group of particles. It then searches for the optimal solution by updating positions of the particles.
In each iteration $t$, the particles are updated by calculating potentially new $pbest_p$ and $lbest_p$ positions which affect the change in current velocity of the particle. $X_p(t)$ will denote the position of particle $p$ at time $t$, i.e. the $t$-th iteration. Similarly, the velocity vector is denoted by $V_p(t)$.

Firstly, the general flow of the PSO algorithm is given, then each functionality is discussed and defined in details.

\begin{center}
    
    \begin{enumerate}
        \item For swarm size s, initialize the group of particles.
        
        \item For each particle: \label{itm:pso_iter}
        \begin{enumerate}
            \item Calculate the fitness value, using the fitness function.
            \item Compare the fitness to its best obtained so far, the better value is stored together with $pbest_p$	
            
            \item Update neighbourhoods and find $lbest_p$ position.
            
            \item Update velocity and position.	
        \end{enumerate}
        
        \item Repeat~\ref{itm:pso_iter}. until maximum number of iterations has been reached.
        
    \end{enumerate}
    
\end{center}



%The remaining part of this chapter is devoted to illustrating problems associated with the objective of this study and more importantly, methods of solving these problems.


%---------------------------------------------------------------------
%---------------------------------------------------------------------
\section{Swarm Initialization}
In order to initialize the PSO algorithm, one must first decide on the size of the swarm (i.e. the number of particles). The swarm will be a static group, meaning that the size will not change through out the computations of a single PSO instance.

The interval for position of all particles must also be chosen. The interval along with the size of the position will essentially define the search space for the swarm.

Additionally, it is convenient to define a constant $v_{max}$ which prevents the particle from traveling in any dimension, further than $v_{max}$ distance in a single iteration. If the velocity was not limited, the jumps in position between iterations could be quite big. This could lead the particles to avoid a big chunk of the search space.


%---------------------------------------------------------------------
%---------------------------------------------------------------------
\section{Neighbourhood}

The neighbourhoods are updated using a simply algorithm based on star topology.
For each particle, neighbourhood is created containing itself and $K$ other random particles. Among that neighbourhood the best particle is found which defines the $lbest$ of the particle of interest.

The neighbourhoods are updated only when there was no change in global best (among all particles in the swarm) since the last iteration.


%---------------------------------------------------------------------
%---------------------------------------------------------------------
\section{Updating the particle}


The following method of updating the particles has its origins in the Standard Particle Swarm Optimization version 2011 presented in~\cite{pso_11}

Let $G_p(t)$ be the centre of gravity of the three points:
\begin{enumerate}
    \item Current position. \\
    $X_p(t)$
    
    \item Point a bit beyond $pbest_p$. \\
    $Y_{p1}(t) = c(pbest_p-X_p(t))$
    
    \item Point a bit beyond $lbest_p$. \\
    $Y_{p2}(t) = c(lbest_p-X_p(t))$
    
\end{enumerate}

The constant $c = \frac{1}{2} + ln(2)$ called a learning factor, together with the inertia parameter that weights the particle's velocity $\omega = \frac{1}{2 * ln(2)}$ used in further equations, was proposed by Clerc in~\cite{pso_anal}.

Formally $G_p(t)$ it is defined by the following formula 
\begin{equation}
G_p(t) = \frac{X_p(t) + Y_{p1}(t) + Y_{p2}(t)} {3}
\end{equation}

We now define a random point $X^{'}_p$ in the hypersphere
\begin{center}
    $\mathcal{H}_p(G_p, d(G_p, X_p))$ 
\end{center}
of centre $G_p$ and of radius $d(G_p, X_p)$ where the function $d$ is an euclidean distance between two points. The time $t$ has been omitted for simplicity.

The velocity update is computed by
\begin{equation}
V_p(t+1) = \omega  V_p(t) + X^{'}_p(t) - X_p(t)
\end{equation}
Thus the position is updated by the equation

\begin{equation}
X_p(t+1) = \omega  V_p(t) + X^{'}_p(t)
\end{equation}

Figure~\ref{fig:pso_sphere} illustrates the idea of updating the position.

\begin{figure}[H]
    \includegraphics[width=0.7\textwidth]{./images/pso_sphere.png}
    \caption{Example of sphere  $\mathcal{H}_p(G_p, d(G_p, X_p))$. Point $x^{'}_{i}$ is chosen randomly within the sphere}
    \label{fig:pso_sphere}
\end{figure}

% Interval confinement
It may happen that the particle might leave the search space, in means that the particle lies outside the acceptable interval. If that happens we generally try to lead the particle back to its right course. For each dimension $x_{d} \in X_p$ that lies  outside the acceptable interval we apply the following:

\[
x_{d} \notin [x_{min}, x_{max}] \Rightarrow \left \{
\begin{array}{ll}
v_{d} = 0 \\
x_d < x_{min} \Rightarrow x_d = x_{min} \\
x_d > x_{max} \Rightarrow x_d = x_{max}
\end{array}
\right.
\]

This means that the corresponding dimension of velocity vector $v_d \in V_p$ is zeroed and the position is bound to the edge of the interval.

\section{Applying PSO}
In order to implement PSO for problem of interest, one must provide specific implementation for two major components of the algorithm. The first one is fitness function which optimizes the solution to a certain goal. The second one is particle representation method allowing to decode the particle into a specific object which can then be used in the fitness function.


\chapter{Interface}

%---------------------------------------------------------------
\begin{thebibliography}{1}
    
    
\end{thebibliography}


\end{document}
